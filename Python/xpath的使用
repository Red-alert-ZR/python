xpath

pip install lxml

from lxml import etree

#网页的源码
html_doc = resp.content.decode("utf-8")
#使用etree去转化html_doc，转化为了一个html的对象，此时element对象可以使用xpath语法
html = etree.HTML(html_doc)
print(html.xpath("xpath语法")

======================

xpath语法

xpath语法中，[1]就是第一个

//a 当前html页面中所有的a标签

//a/@href 当前html页面中所有a标签中的href的属性内容

//a/text() 当前html页面中所有a标签中的文本内容

//img/@src 拿到所有img标签中的src的内容

//a//img/@src 拿到所有a标签下面的所有img标签中的src内容

//img[@alt='猪猪侠之南海日记2']/@src 选择img标签中alt是某个值的标签并且提取src值

//div[@class='lst-item'][3]/a[4]/img/@src 选取页面中所有div并且class是lst-item的，选择其中第3个，并且在此之下，选择第4个a标签中的img标签的src属性内容

//div[@class='lst-item'][1]/a[last()]/img 这里的a标签是选择最后一个

//p[text()='超变武兽第二季'] 查找所有p标签中，文字内容是xxx的

//p[contains(text(),'蜡笔')] 查找所有p标签中，包含某些文字的

//p[@*] 页面中所有p标签凡是带有属性的都选出来

//div[@class='lst-item'][2]/* 这里的*代表选择所有的元素

//div[@class='m-lst']/div[position()=4] 选择页面中所有class是m-lst的div中的第4个div

-----------------------------------
import requests
from lxml import etree

#页面内爬虫
def pachong(url):
    url = url
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36"
    }
    resp = requests.get(url=url, headers=headers)
    # 网页的源码
    html_doc = resp.content.decode("utf-8")
    # 使用etree去转化html_doc，转化为了一个html的对象，此时element对象可以使用xpath语法
    html = etree.HTML(html_doc)
    dongmantitle = html.xpath("//div[@class='u-ct']/p[@class='u-tt']/text()")
    dongmanpic = html.xpath("//div[@class='lst']/a/img/@data-src")
    print(dongmantitle)
    print(dongmanpic)
    pass

#发现下一页的爬虫
def find_next_page(url):
    url = url
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36"
    }
    resp = requests.get(url=url, headers=headers)
    # 网页的源码
    html_doc = resp.content.decode("utf-8")
    # 使用etree去转化html_doc，转化为了一个html的对象，此时element对象可以使用xpath语法
    html = etree.HTML(html_doc)
    # 获得下一页的链接
    next_page = html.xpath("//a[contains(text(),'下一页')]/@href")
    # 创建完整的链接
    really_next_page = "http://www.4399dmw.com" + next_page[0]
    return really_next_page

def main():
    url = "http://www.4399dmw.com/search/dh-1-0-0-0-0-1-0/"
    while True:
        try:
            print("开始爬行的url："+url)
            pachong(url)
            url = find_next_page(url)
        except:
            break

    print("最后一页也爬行完了")
    pass

===============================

反爬虫：

通过user-agent

"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36"

通过referer

通过cookie
考虑建立cookie池（多账户）

通过ip
使用代理ip等方法

验证码
考虑写程序识别验证码，打码平台

通过自定义字体
1.有的网站通过审查元素字体无法显示，显示为口，考虑使用手机网络访问

2.通过字体偏移
ABCDEFG....
CDEFGHI....

========================

selenium

直接控制浏览器的插件

pip install selenium

web自动化测试工具
https://npm.taobao.org

chromedriver

https://vikyd.github.io/download-chromium-history-version/#/
